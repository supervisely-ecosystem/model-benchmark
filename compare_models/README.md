<div align="center" markdown>

# Model Comparison

<p align="center">
  <a href="#Overview">Overview</a> •
  <a href="#Preparation">Preparation</a> •
  <a href="#How-To-Run">How To Run</a>
</p>


[![](https://img.shields.io/badge/supervisely-ecosystem-brightgreen)](https://ecosystem.supervisely.com/apps/supervisely-ecosystem/model-benchmark/compare_models)
[![](https://img.shields.io/badge/slack-chat-green.svg?logo=slack)](https://supervisely.com/slack)
![GitHub release (latest SemVer)](https://img.shields.io/github/v/release/supervisely-ecosystem/model-benchmark/compare_models)
[![views](https://app.supervisely.com/img/badges/views/supervisely-ecosystem/model-benchmark/compare_models.png)](https://supervisely.com)
[![runs](https://app.supervisely.com/img/badges/runs/supervisely-ecosystem/model-benchmark/compare_models.png)](https://supervisely.com)

</div>

## Overview

The Model Comparison app is designed to compare results of multiple models evaluated on the same Ground Truth project. It consumes the evaluation outputs produced by the main "Evaluator for Model Benchmark" app or training sessions and generates a side‑by‑side report of key metrics across selected models.

Key capabilities:

- Compare core quality metrics (e.g., mAP, precision, recall, F1) across models
- Summary table with quick links to detailed per‑model reports
- Selection of multiple evaluation folders at once
- Easy access to the comparison report via a link in the task output
- Comprehensive visualization of results for intuitive analysis

## Preparation

Before running, make sure that:

- You have evaluation results generated in training sessions or generated by the main "Evaluator for Model Benchmark" app.
- All evaluations were produced on the same Ground Truth project and share the same class set and validation dataset.

## How To Run

<img src="https://github.com/supervisely-ecosystem/model-benchmark/releases/download/v0.0.4/compare_models.jpg"/>

**Step 1:** Open "Experiments" page and select experiments to compare.

**Step 2:** Click the "with selected" button and choose "Compare Model Evaluation".

Note: All selected evaluations must correspond to the same GT project and class set; otherwise, the comparison may be invalid.
