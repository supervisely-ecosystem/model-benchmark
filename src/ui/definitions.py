true_positives = "True Positives (TP): These are correctly detected objects. For a prediction to be counted as a true positive, the predicted bounding box must align with a ground truth bounding box with an Intersection over Union (IoU) of 0.5 or more, and the object must be correctly classified"
false_positives = "False Positives (FP): These are incorrect detections made by the model. They occur when the model predicts a bounding box that either does not overlap sufficiently with any ground truth box (IoU less than 0.5) or incorrectly classifies the object within the bounding box. For example, the model detects a car in the image, but there is no car in the ground truth."
false_negatives = "False Negatives (FN): These are the missed detections. They occur when an actual object in the ground truth is not detected by the model, meaning there is no predicted bounding box with an IoU of 0.5 or more for this object. For example, there is a car in the image, but the model fails to detect it."

confidence_threshold = "Confidence threshold is a hyperparameter used to filter out predictions that the model is not confident in. It helps to control the trade-off between precision and recall in the model's output. By setting a higher confidence threshold, you ensure that only the most certain predictions are considered, thereby reducing the number of false predictions."
confidence_score = "The confidence score, also known as probability score, quantifies how confident the model is that its prediction is correct. It is a numerical value between 0 and 1, generated by the model for each bounding box, that represents the likelihood that a predicted bounding box contains an object of a particular class. "

f1_score = "F1 Score is the harmonic mean of precision and recall. It is a useful metric when you need to balance precision and recall. It is calculated as 2 * (precision * recall) / (precision + recall)."
average_precision = "Average precision (AP) is computed as the area under the precision-recall curve. It measures the precision of the model at different recall levels and provides a single number that summarizes the trade-off between precision and recall for a given class."

about_pr_tradeoffs = "A system with high recall but low precision returns many results, but most of its predictions are incorrect or redundant (false positive). A system with high precision but low recall is just the opposite, returning very few results, most of its predictions are correct. An ideal system with high precision and high recall will return many results, with all results predicted correctly."

iou_score = "IoU score is a measure of overlap between predicted bounding box and ground truth bounding box. A higher IoU score indicates better alignment between the predicted and ground truth bounding boxes."
iou_threshold = "The IoU threshold is a predefined value (set at 0.5 in many benchmarks) that determines the minimum acceptable IoU score for a predicted bounding box to be considered a correct detection. When the IoU of a predicted bounding box and an actual bounding box is below this threshold, the prediction is considered a false positive. Higher IoU thresholds require more precise localization, which can lead to lower metrics if the model's predictions are less accurate."


# <abbr title="{definitions.true_positives}">True Positives</abbr>