import os
import random
from collections import defaultdict

import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
from matplotlib import pyplot as plt
from pycocotools.coco import COCO
from pycocotools.cocoeval import COCOeval, Params

import src.globals as g
from src.ui import definitions
import supervisely as sly
from supervisely.app.widgets import (
    Button,
    Card,
    Collapse,
    Container,
    DatasetThumbnail,
    IFrame,
    Markdown,
    NotificationBox,
    SelectDataset,
    Text,
)
from supervisely.nn.benchmark import metric_provider
from supervisely.nn.benchmark.metric_provider import METRIC_NAMES, MetricProvider


def iou_distribution():
    fig = go.Figure()
    nbins = 40
    fig.add_trace(go.Histogram(x=g.m.ious, nbinsx=nbins))
    fig.update_layout(
        # title="IoU Distribution",
        xaxis_title="IoU",
        yaxis_title="Count",
        width=600,
        height=500,
    )

    # Add annotation for mean IoU as vertical line
    mean_iou = g.m.ious.mean()
    y1 = len(g.m.ious) // nbins
    fig.add_shape(
        type="line",
        x0=mean_iou,
        x1=mean_iou,
        y0=0,
        y1=y1,
        line=dict(color="orange", width=2, dash="dash"),
    )
    fig.add_annotation(x=mean_iou, y=y1, text=f"Mean IoU: {mean_iou:.2f}", showarrow=False)
    # fig.show()
    return fig


base_metrics = g.m.base_metrics()

markdown_localization_accuracy = Markdown(
    f"""
## Localization Accuracy (IoU)

This section measures how closely predicted bounding boxes generated by the model are aligned with the actual (ground truth) bounding boxes.
""",
    show_border=False,
    height=80,
)
notibox_avg_iou = NotificationBox(f"Avg. IoU = {base_metrics['iou']:.2f}")

text_image = Text(
    "<img src='https://github.com/dataset-ninja/model-benchmark-template/assets/78355358/8d7c63d0-2f3b-4f3f-9fd8-c6383a4bfba4' alt='alt text' width='300' />"
)
text_info = Text(
    "To measure it, we calculate the <b>Intersection over Union (IoU)</b>. Intuitively, the higher the IoU, the closer two bounding boxes are. IoU is calculated by dividing the <b>area of overlap</b> between the predicted bounding box and the ground truth bounding box by the <b>area of union</b> of these two boxes.",
    "info",
)
collapsables = Collapse(
    [
        Collapse.Item(
            "How IoU is calculated?",
            "How IoU is calculated?",
            Container([text_image, text_info]),
        )
    ]
)

markdown_iou_distribution = Markdown(
    f"""
### IoU Distribution

This histogram represents the distribution of <abbr title="{definitions.iou_score}">IoU scores</abbr> between all predictions and their matched ground truth objects. This gives you a sense of how well the model aligns bounding boxes. Ideally, if the model aligns boxes very well, rightmost bars (from 0.9 to 1.0 IoU) should be much higher than others.
""",
    show_border=False,
)
iframe_iou_distribution = IFrame("static/10_iou_distribution.html", width=620, height=520)
container = Container(
    widgets=[
        markdown_localization_accuracy,
        collapsables,
        markdown_iou_distribution,
        notibox_avg_iou,
        iframe_iou_distribution,
    ]
)
